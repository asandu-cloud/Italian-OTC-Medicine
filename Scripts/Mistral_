import os
import re
import pickle
import warnings
import numpy as np
import torch
from typing import List, Dict
from tqdm.auto import tqdm
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import hnswlib
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# Filtra warning non necessari
warnings.filterwarnings('ignore')

# Configurazione Tesseract (se necessario su Windows)
import pytesseract
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

print('âœ… Librerie importate correttamente.')

class Config:
    # TODO: Sarebbe meglio usare percorsi relativi o .env in futuro
    PDF_FOLDER = r"C:\Users\gabri\Documents\Luiss\Luiss - Year Two\Advanced AI - LLM\Med M1\Medicinali Car"
    CACHE_DIR = r"C:\Users\gabri\Documents\Luiss\Luiss - Year Two\Advanced AI - LLM\Med M1\rag_cache"
    
    # Nomi dei file di cache
    EMBEDDINGS_FILE = os.path.join(CACHE_DIR, 'aifa_embeddings.npy')
    CHUNKS_FILE = os.path.join(CACHE_DIR, 'chunks.pkl')
    IDX_CACHE = os.path.join(CACHE_DIR, 'hnswlib_index.bin')

    # Configurazioni modello e chunking
    EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 50

    TOP_K = 10                  
    SIMILARITY_THRESHOLD = 0.3
    BATCH_SIZE = 32
    VERBOSE = True

config = Config()

# Crea le cartelle se non esistono
os.makedirs(config.CACHE_DIR, exist_ok=True)


# ==========================================
# 1. GESTIONE TESTO E PDF
# ==========================================

def clean_text(text: str) -> str:
    """Pulisce il testo rimuovendo caratteri non stampabili"""
    if not text or not isinstance(text, str):
        return ''
    cleaned = ''.join(c for c in text if c.isprintable() or c == '\n')
    cleaned = ' '.join(cleaned.split())
    return cleaned.strip()

def extract_text_from_pdf(pdf_path: str) -> str:
    """Estrae testo pulito da un PDF."""
    try:
        reader = PdfReader(pdf_path)
        content = []
        for page in reader.pages:
            try:
                text = page.extract_text()
                if text and len(text.strip()) > 20:
                    cleaned = clean_text(text)
                    if cleaned:
                        content.append(cleaned)
            except:
                continue
        return '\n'.join(content).strip()
    except Exception as e:
        print(f'âš ï¸ Errore lettura PDF {pdf_path}: {e}')
        return ''

def extract_and_chunk_all_pdfs(config) -> List[Dict]:
    """Estrae e divide in chunk tutti i PDF nella cartella configurata"""
    chunker = RecursiveCharacterTextSplitter(
        chunk_size=config.CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP
    )

    if not os.path.exists(config.PDF_FOLDER):
        print(f"âŒ Errore: La cartella {config.PDF_FOLDER} non esiste.")
        return []

    pdf_files = [f for f in os.listdir(config.PDF_FOLDER) if f.endswith('.pdf')]
    print(f'ðŸ“š Trovati {len(pdf_files)} PDF da processare')

    all_chunks = []
    
    for pdf_file in tqdm(pdf_files, desc='ðŸ“„ Processamento PDF'):
        file_path = os.path.join(config.PDF_FOLDER, pdf_file)
        raw_text = extract_text_from_pdf(file_path)

        if raw_text and len(raw_text.strip()) > 100:
            text_chunks = chunker.split_text(raw_text)
            for idx, chunk_text in enumerate(text_chunks):
                all_chunks.append({
                    'text': chunk_text,
                    'document': pdf_file,
                    'chunk_id': f'{pdf_file}_{idx}'
                })
    
    return all_chunks

    # ==========================================
# 2. GESTIONE EMBEDDINGS E INDICE
# ==========================================

def load_or_create_index(chunks):
    """Carica o crea l'indice HNSWlib e gli embeddings"""
    
    # Carica modello embedding (su CPU per compatibilitÃ )
    print(f'ðŸ“¥ Caricamento modello embedding: {config.EMBEDDING_MODEL}')
    embedder = SentenceTransformer(config.EMBEDDING_MODEL)
    embedder.to('cpu')

    if os.path.exists(config.EMBEDDINGS_FILE) and os.path.exists(config.IDX_CACHE):
        print('ðŸ“¦ Caricamento indice dalla cache...')
        embeddings = np.load(config.EMBEDDINGS_FILE)
        
        # Inizializza indice vuoto per poi caricarlo
        dim = embeddings.shape[1]
        index = hnswlib.Index(space='cosine', dim=dim)
        index.load_index(config.IDX_CACHE, max_elements=embeddings.shape[0])
    else:
        print('ðŸ”„ Creazione nuovi embeddings...')
        documents = [chunk['text'] for chunk in chunks]
        embeddings = embedder.encode(documents, show_progress_bar=True, batch_size=config.BATCH_SIZE, device='cpu')
        
        # Crea indice HNSWlib
        dim = embeddings.shape[1]
        num_elements = len(documents)
        index = hnswlib.Index(space='cosine', dim=dim)
        index.init_index(max_elements=num_elements, ef_construction=200, M=16)
        index.add_items(embeddings, np.arange(num_elements))
        
        # Salva
        np.save(config.EMBEDDINGS_FILE, embeddings)
        index.save_index(config.IDX_CACHE)
        
    return index, embedder

    # ==========================================
# 3. GESTIONE LLM (MISTRAL 7B)
# ==========================================

def load_llm():
    """Carica il modello Mistral 7B quantizzato"""
    print('ðŸ¤– Caricamento Mistral 7B...')
    
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )

    model_name = "mistralai/Mistral-7B-Instruct-v0.2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        low_cpu_mem_usage=True
    )
    
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_tokens=512):
    """Genera una risposta usando il modello caricato"""
    messages = [{"role": "user", "content": prompt}]
    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response.strip()