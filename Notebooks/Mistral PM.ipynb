{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e20810c",
   "metadata": {},
   "source": [
    "# Mistral\n",
    "\n",
    "previous model (pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b67d3",
   "metadata": {},
   "source": [
    "1) Setting Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1b16c",
   "metadata": {},
   "source": [
    "run only once the first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae815970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Standard libraries \n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import List, Dict \n",
    "\n",
    "# 2. Third Party libraries \n",
    "import numpy as np \n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hnswlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries correctly installed and imported\")\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT = Path.cwd().parent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b427231",
   "metadata": {},
   "source": [
    "2) Class Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d02091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # TODO: Sarebbe meglio usare percorsi relativi o .env in futuro\n",
    "    PDF_FOLDER = r\"C:\\Users\\gabri\\Documents\\Luiss\\Luiss - Year Two\\Advanced AI - LLM\\Med M1\\Medicinali Car\"\n",
    "    CACHE_DIR = r\"C:\\Users\\gabri\\Documents\\Luiss\\Luiss - Year Two\\Advanced AI - LLM\\Med M1\\rag_cache\"\n",
    "    \n",
    "    # Nomi dei file di cache\n",
    "    EMBEDDINGS_FILE = os.path.join(CACHE_DIR, 'aifa_embeddings.npy')\n",
    "    CHUNKS_FILE = os.path.join(CACHE_DIR, 'chunks.pkl')\n",
    "    IDX_CACHE = os.path.join(CACHE_DIR, 'hnswlib_index.bin')\n",
    "\n",
    "    # Configurazioni modello e chunking\n",
    "    EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    CHUNK_SIZE = 500\n",
    "    CHUNK_OVERLAP = 50\n",
    "\n",
    "    TOP_K = 10                  \n",
    "    SIMILARITY_THRESHOLD = 0.3\n",
    "    BATCH_SIZE = 32\n",
    "    VERBOSE = True\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Crea le cartelle se non esistono\n",
    "os.makedirs(config.CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d0e40",
   "metadata": {},
   "source": [
    "3) Text and PDF management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041117af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Cleans text by removing non-printable characters\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return ''\n",
    "    cleaned = ''.join(c for c in text if c.isprintable() or c == '\\n')\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts clean text from a PDF.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        content = []\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                text = page.extract_text()\n",
    "                if text and len(text.strip()) > 20:\n",
    "                    cleaned = clean_text(text)\n",
    "                    if cleaned:\n",
    "                        content.append(cleaned)\n",
    "            except:\n",
    "                continue\n",
    "        return '\\n'.join(content).strip()\n",
    "    except Exception as e:\n",
    "        print(f'Error reading PDF {pdf_path}: {e}')\n",
    "        return ''\n",
    "\n",
    "def extract_and_chunk_all_pdfs(config) -> List[Dict]:\n",
    "    \"\"\"Extracts and chunks all PDFs in the configured folder\"\"\"\n",
    "    chunker = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config.CHUNK_SIZE,\n",
    "        chunk_overlap=config.CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(config.PDF_FOLDER):\n",
    "        print(f\"Error: The folder {config.PDF_FOLDER} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(config.PDF_FOLDER) if f.endswith('.pdf')]\n",
    "    print(f'Found {len(pdf_files)} PDFs to process')\n",
    "    all_chunks = []\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc='ðŸ“„ Processing PDFs'):\n",
    "        file_path = os.path.join(config.PDF_FOLDER, pdf_file)\n",
    "        raw_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "        if raw_text and len(raw_text.strip()) > 100:\n",
    "            text_chunks = chunker.split_text(raw_text)\n",
    "            for idx, chunk_text in enumerate(text_chunks):\n",
    "                all_chunks.append({\n",
    "                    'text': chunk_text,\n",
    "                    'document': pdf_file,\n",
    "                    'chunk_id': f'{pdf_file}_{idx}'\n",
    "                })\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d9ded",
   "metadata": {},
   "source": [
    "3) Embeddings and Index Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8811a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_index(chunks):\n",
    "    \"\"\"Upload or create the HNSWlib index and embeddings\"\"\"\n",
    "    \n",
    "    # Load embedding model (on CPU for compatibility)\n",
    "    print(f'Loading embedding model: {config.EMBEDDING_MODEL}')\n",
    "    embedder = SentenceTransformer(config.EMBEDDING_MODEL)\n",
    "    embedder.to('cpu')\n",
    "\n",
    "    if os.path.exists(config.EMBEDDINGS_FILE) and os.path.exists(config.IDX_CACHE):\n",
    "        print('Loading index from cache...')\n",
    "        embeddings = np.load(config.EMBEDDINGS_FILE)\n",
    "        \n",
    "        # Initialize empty index to then load it\n",
    "        dim = embeddings.shape[1]\n",
    "        index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        index.load_index(config.IDX_CACHE, max_elements=embeddings.shape[0])\n",
    "    else:\n",
    "        print('Creating new embeddings...')\n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        embeddings = embedder.encode(documents, show_progress_bar=True, batch_size=config.BATCH_SIZE, device='cpu')\n",
    "        \n",
    "        # Create HNSWlib index\n",
    "        dim = embeddings.shape[1]\n",
    "        num_elements = len(documents)\n",
    "        index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        index.init_index(max_elements=num_elements, ef_construction=200, M=16)\n",
    "        index.add_items(embeddings, np.arange(num_elements))\n",
    "        \n",
    "        # Save\n",
    "        np.save(config.EMBEDDINGS_FILE, embeddings)\n",
    "        index.save_index(config.IDX_CACHE)\n",
    "        \n",
    "    return index, embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d88843",
   "metadata": {},
   "source": [
    "4) LLM Management (Mistral 7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619744b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    \"\"\"Upload the quantized Mistral 7B model\"\"\"\n",
    "    print('Loading Mistral 7B...')\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_tokens=512):\n",
    "    \"\"\"Generate a response using the loaded model\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6997c1e",
   "metadata": {},
   "source": [
    "5) RAG System and Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, index, chunks, embedder):\n",
    "    \"\"\"Retrieve the most relevant chunks for the query\"\"\"\n",
    "    query_vec = embedder.encode([query], device='cpu')\n",
    "    indices, distances = index.knn_query(query_vec, k=config.TOP_K)\n",
    "    \n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        similarity = 1.0 - dist\n",
    "        if similarity >= config.SIMILARITY_THRESHOLD:\n",
    "            results.append(chunks[idx])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def rag_chat_system():\n",
    "    \"\"\"Main function that starts the system\"\"\"\n",
    "    \n",
    "    chunks = extract_and_chunk_all_pdfs(config)\n",
    "    if not chunks:\n",
    "        return\n",
    "        \n",
    "    index, embedder = load_or_create_index(chunks)\n",
    "    \n",
    "    print(\"System ready! (Retrieval-only mode for testing)\")\n",
    "\n",
    "    print('\\n' + '='*50)\n",
    "    print('CHATBOT MEDICO AIFA - Type \"exit\" to quit')\n",
    "    print('='*50)\n",
    "\n",
    "    while True:\n",
    "        query = input('\\nYou: ').strip()\n",
    "        if query.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "            \n",
    "        # Retrieval\n",
    "        context_chunks = retrieve_context(query, index, chunks, embedder)\n",
    "        \n",
    "        if not context_chunks:\n",
    "            print(\"Assistant: I couldn't find relevant information in the documents.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n Found {len(context_chunks)} relevant documents.\")\n",
    "        \n",
    "        best_match = context_chunks[0]\n",
    "        print(f\"Source: {best_match['document']}\")\n",
    "        print(f\"Content: {best_match['text'][:300]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rag_chat_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.22)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
