{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e20810c",
   "metadata": {},
   "source": [
    "# Mistral\n",
    "\n",
    "previous model (pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b67d3",
   "metadata": {},
   "source": [
    "1) Setting Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1b16c",
   "metadata": {},
   "source": [
    "run only once this first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af9bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy\n",
    "!pip install -q torch\n",
    "!pip install -q tqdm\n",
    "!pip install -q PyPDF2\n",
    "!pip install -q langchain_text_splitters\n",
    "!pip install -q sentence_transformers\n",
    "!pip install -q hnswlib\n",
    "!pip install -q transformers\n",
    "!pip install -q pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae815970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries correctly installed and imported\n"
     ]
    }
   ],
   "source": [
    "# 1.  Standard libraries \n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import List, Dict \n",
    "\n",
    "# 2. Third Party libraries \n",
    "import numpy as np \n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hnswlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries correctly installed and imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b427231",
   "metadata": {},
   "source": [
    "2) Class Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d02091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # TODO: Sarebbe meglio usare percorsi relativi o .env in futuro\n",
    "    PDF_FOLDER = r\"C:\\Users\\gabri\\Documents\\Luiss\\Luiss - Year Two\\Advanced AI - LLM\\Med M1\\Medicinali Car\"\n",
    "    CACHE_DIR = r\"C:\\Users\\gabri\\Documents\\Luiss\\Luiss - Year Two\\Advanced AI - LLM\\Med M1\\rag_cache\"\n",
    "    \n",
    "    # Nomi dei file di cache\n",
    "    EMBEDDINGS_FILE = os.path.join(CACHE_DIR, 'aifa_embeddings.npy')\n",
    "    CHUNKS_FILE = os.path.join(CACHE_DIR, 'chunks.pkl')\n",
    "    IDX_CACHE = os.path.join(CACHE_DIR, 'hnswlib_index.bin')\n",
    "\n",
    "    # Configurazioni modello e chunking\n",
    "    EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    CHUNK_SIZE = 500\n",
    "    CHUNK_OVERLAP = 50\n",
    "\n",
    "    TOP_K = 10                  \n",
    "    SIMILARITY_THRESHOLD = 0.3\n",
    "    BATCH_SIZE = 32\n",
    "    VERBOSE = True\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Crea le cartelle se non esistono\n",
    "os.makedirs(config.CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d0e40",
   "metadata": {},
   "source": [
    "3) Text and PDF management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041117af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Cleans text by removing non-printable characters\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return ''\n",
    "    cleaned = ''.join(c for c in text if c.isprintable() or c == '\\n')\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts clean text from a PDF.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        content = []\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                text = page.extract_text()\n",
    "                if text and len(text.strip()) > 20:\n",
    "                    cleaned = clean_text(text)\n",
    "                    if cleaned:\n",
    "                        content.append(cleaned)\n",
    "            except:\n",
    "                continue\n",
    "        return '\\n'.join(content).strip()\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸ Error reading PDF {pdf_path}: {e}')\n",
    "        return ''\n",
    "\n",
    "def extract_and_chunk_all_pdfs(config) -> List[Dict]:\n",
    "    \"\"\"Extracts and chunks all PDFs in the configured folder\"\"\"\n",
    "    chunker = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config.CHUNK_SIZE,\n",
    "        chunk_overlap=config.CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(config.PDF_FOLDER):\n",
    "        print(f\"âŒ Error: The folder {config.PDF_FOLDER} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(config.PDF_FOLDER) if f.endswith('.pdf')]\n",
    "    print(f'ðŸ“š Found {len(pdf_files)} PDFs to process')\n",
    "    all_chunks = []\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc='ðŸ“„ Processing PDFs'):\n",
    "        file_path = os.path.join(config.PDF_FOLDER, pdf_file)\n",
    "        raw_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "        if raw_text and len(raw_text.strip()) > 100:\n",
    "            text_chunks = chunker.split_text(raw_text)\n",
    "            for idx, chunk_text in enumerate(text_chunks):\n",
    "                all_chunks.append({\n",
    "                    'text': chunk_text,\n",
    "                    'document': pdf_file,\n",
    "                    'chunk_id': f'{pdf_file}_{idx}'\n",
    "                })\n",
    "    \n",
    "    return all_chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
