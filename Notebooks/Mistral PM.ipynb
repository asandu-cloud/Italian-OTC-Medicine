{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e20810c",
   "metadata": {},
   "source": [
    "# Mistral\n",
    "\n",
    "previous model (pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b67d3",
   "metadata": {},
   "source": [
    "1) Setting Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1b16c",
   "metadata": {},
   "source": [
    "run only once this first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af9bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy\n",
    "!pip install -q torch\n",
    "!pip install -q tqdm\n",
    "!pip install -q PyPDF2\n",
    "!pip install -q langchain_text_splitters\n",
    "!pip install -q sentence_transformers\n",
    "!pip install -q hnswlib\n",
    "!pip install -q transformers\n",
    "!pip install -q pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae815970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries correctly installed and imported\n"
     ]
    }
   ],
   "source": [
    "# 1.  Standard libraries \n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import List, Dict \n",
    "\n",
    "# 2. Third Party libraries \n",
    "import numpy as np \n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hnswlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries correctly installed and imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b427231",
   "metadata": {},
   "source": [
    "2) Class Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d02091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # TODO: Sarebbe meglio usare percorsi relativi o .env in futuro\n",
    "    PDF_FOLDER = r\"C:\\Users\\gabri\\Documents\\Luiss\\Luiss - Year Two\\Advanced AI - LLM\\Med M1\\Medicinali Car\"\n",
    "    CACHE_DIR = r\"C:\\Users\\gabri\\Documents\\Luiss\\Luiss - Year Two\\Advanced AI - LLM\\Med M1\\rag_cache\"\n",
    "    \n",
    "    # Nomi dei file di cache\n",
    "    EMBEDDINGS_FILE = os.path.join(CACHE_DIR, 'aifa_embeddings.npy')\n",
    "    CHUNKS_FILE = os.path.join(CACHE_DIR, 'chunks.pkl')\n",
    "    IDX_CACHE = os.path.join(CACHE_DIR, 'hnswlib_index.bin')\n",
    "\n",
    "    # Configurazioni modello e chunking\n",
    "    EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    CHUNK_SIZE = 500\n",
    "    CHUNK_OVERLAP = 50\n",
    "\n",
    "    TOP_K = 10                  \n",
    "    SIMILARITY_THRESHOLD = 0.3\n",
    "    BATCH_SIZE = 32\n",
    "    VERBOSE = True\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Crea le cartelle se non esistono\n",
    "os.makedirs(config.CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d0e40",
   "metadata": {},
   "source": [
    "3) Text and PDF management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041117af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Cleans text by removing non-printable characters\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return ''\n",
    "    cleaned = ''.join(c for c in text if c.isprintable() or c == '\\n')\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts clean text from a PDF.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        content = []\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                text = page.extract_text()\n",
    "                if text and len(text.strip()) > 20:\n",
    "                    cleaned = clean_text(text)\n",
    "                    if cleaned:\n",
    "                        content.append(cleaned)\n",
    "            except:\n",
    "                continue\n",
    "        return '\\n'.join(content).strip()\n",
    "    except Exception as e:\n",
    "        print(f'‚ö†Ô∏è Error reading PDF {pdf_path}: {e}')\n",
    "        return ''\n",
    "\n",
    "def extract_and_chunk_all_pdfs(config) -> List[Dict]:\n",
    "    \"\"\"Extracts and chunks all PDFs in the configured folder\"\"\"\n",
    "    chunker = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config.CHUNK_SIZE,\n",
    "        chunk_overlap=config.CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(config.PDF_FOLDER):\n",
    "        print(f\"‚ùå Error: The folder {config.PDF_FOLDER} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(config.PDF_FOLDER) if f.endswith('.pdf')]\n",
    "    print(f'üìö Found {len(pdf_files)} PDFs to process')\n",
    "    all_chunks = []\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc='üìÑ Processing PDFs'):\n",
    "        file_path = os.path.join(config.PDF_FOLDER, pdf_file)\n",
    "        raw_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "        if raw_text and len(raw_text.strip()) > 100:\n",
    "            text_chunks = chunker.split_text(raw_text)\n",
    "            for idx, chunk_text in enumerate(text_chunks):\n",
    "                all_chunks.append({\n",
    "                    'text': chunk_text,\n",
    "                    'document': pdf_file,\n",
    "                    'chunk_id': f'{pdf_file}_{idx}'\n",
    "                })\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d9ded",
   "metadata": {},
   "source": [
    "3) Embeddings and Index Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8811a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_index(chunks):\n",
    "    \"\"\"Upload or create the HNSWlib index and embeddings\"\"\"\n",
    "    \n",
    "    # Load embedding model (on CPU for compatibility)\n",
    "    print(f'üì• Loading embedding model: {config.EMBEDDING_MODEL}')\n",
    "    embedder = SentenceTransformer(config.EMBEDDING_MODEL)\n",
    "    embedder.to('cpu')\n",
    "\n",
    "    if os.path.exists(config.EMBEDDINGS_FILE) and os.path.exists(config.IDX_CACHE):\n",
    "        print('üì¶ Loading index from cache...')\n",
    "        embeddings = np.load(config.EMBEDDINGS_FILE)\n",
    "        \n",
    "        # Initialize empty index to then load it\n",
    "        dim = embeddings.shape[1]\n",
    "        index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        index.load_index(config.IDX_CACHE, max_elements=embeddings.shape[0])\n",
    "    else:\n",
    "        print('üîÑ Creating new embeddings...')\n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        embeddings = embedder.encode(documents, show_progress_bar=True, batch_size=config.BATCH_SIZE, device='cpu')\n",
    "        \n",
    "        # Create HNSWlib index\n",
    "        dim = embeddings.shape[1]\n",
    "        num_elements = len(documents)\n",
    "        index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        index.init_index(max_elements=num_elements, ef_construction=200, M=16)\n",
    "        index.add_items(embeddings, np.arange(num_elements))\n",
    "        \n",
    "        # Save\n",
    "        np.save(config.EMBEDDINGS_FILE, embeddings)\n",
    "        index.save_index(config.IDX_CACHE)\n",
    "        \n",
    "    return index, embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d88843",
   "metadata": {},
   "source": [
    "4) LLM Management (Mistral 7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "619744b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    \"\"\"Upload the quantized Mistral 7B model\"\"\"\n",
    "    print('Loading Mistral 7B...')\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_tokens=512):\n",
    "    \"\"\"Generate a response using the loaded model\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6997c1e",
   "metadata": {},
   "source": [
    "5) RAG System and Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc5b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 47 PDFs to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìÑ Processing PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:48<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading embedding model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "üì¶ Loading index from cache...\n",
      "System ready! (Retrieval-only mode for testing)\n",
      "\n",
      "==================================================\n",
      "üí¨ CHATBOT MEDICO AIFA - Type \"exit\" to quit\n",
      "==================================================\n",
      "\n",
      " Found 10 relevant documents.\n",
      "üìÑ Source: FI_000219_012745.pdf\n",
      "üìù Content: indesiderati 5. Come conservare Tachipirina 6. Contenuto della confezione e a ltre informazioni 1. Che cos‚Äô √® Tachipirina e a cosa serve Tachipirina √® uno sciroppo per uso orale contenente il principio attivo paracetamolo che agisce r iducendo la febbre (antipiretico) e alleviando il dolore (analges...\n",
      "\n",
      " Found 4 relevant documents.\n",
      "üìÑ Source: FI_000219_012745.pdf\n",
      "üìù Content: sospensione √® dotata di un tappo di sicurezza. Le istruzioni per l‚Äôapertura e la chiusur a sono di seguito riportate: Per aprire : Per chiudere : premere avvitare a fondo contemporaneamente premendo girare ‚û¢ Dopo aver svitato il tappo, spingendolo verso il basso e contemporaneamente girando verso si...\n"
     ]
    }
   ],
   "source": [
    "def retrieve_context(query, index, chunks, embedder):\n",
    "    \"\"\"Retrieve the most relevant chunks for the query\"\"\"\n",
    "    query_vec = embedder.encode([query], device='cpu')\n",
    "    indices, distances = index.knn_query(query_vec, k=config.TOP_K)\n",
    "    \n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        similarity = 1.0 - dist\n",
    "        if similarity >= config.SIMILARITY_THRESHOLD:\n",
    "            results.append(chunks[idx])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def rag_chat_system():\n",
    "    \"\"\"Main function that starts the system\"\"\"\n",
    "    \n",
    "    chunks = extract_and_chunk_all_pdfs(config)\n",
    "    if not chunks:\n",
    "        return\n",
    "        \n",
    "    index, embedder = load_or_create_index(chunks)\n",
    "    \n",
    "    print(\"System ready! (Retrieval-only mode for testing)\")\n",
    "\n",
    "    print('\\n' + '='*50)\n",
    "    print('üí¨ CHATBOT MEDICO AIFA - Type \"exit\" to quit')\n",
    "    print('='*50)\n",
    "\n",
    "    while True:\n",
    "        query = input('\\nüôã You: ').strip()\n",
    "        if query.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "            \n",
    "        # Retrieval\n",
    "        context_chunks = retrieve_context(query, index, chunks, embedder)\n",
    "        \n",
    "        if not context_chunks:\n",
    "            print(\"ü§ñ Assistant: I couldn't find relevant information in the documents.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n Found {len(context_chunks)} relevant documents.\")\n",
    "        \n",
    "        best_match = context_chunks[0]\n",
    "        print(f\"üìÑ Source: {best_match['document']}\")\n",
    "        print(f\"üìù Content: {best_match['text'][:300]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rag_chat_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
