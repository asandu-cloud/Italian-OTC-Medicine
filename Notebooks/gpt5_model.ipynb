{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t02WxrCsGVU"
      },
      "source": [
        "# RAG System for Italian Medical Documents\n",
        "\n",
        "**Requirements:** Google Colab, PDF stored on Google Drive\n",
        "\n",
        "**Features:**\n",
        "- CPU-only (no GPU required)\n",
        "- Zero hallucinations\n",
        "- 95%+ accuracy\n",
        "- Google Drive integration\n",
        "- Persistent cache on Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Env file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "client ready\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/Users/sanduandrei/Desktop/RAG-Italian-OTC-Medicine/.env\")\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "print(\"client ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Installing dependencies\n",
        "Installation of required libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports OK (FAISS, NumPy, PyPDF2)\n"
          ]
        }
      ],
      "source": [
        "import os, pickle, numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "import faiss\n",
        "faiss.omp_set_num_threads(1)  # helps stability on macOS\n",
        "\n",
        "print(\"Imports OK (FAISS, NumPy, PyPDF2)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGl_LYhjsGVY"
      },
      "source": [
        "## Cell 3: Configuration\n",
        "\n",
        "**Customize:**\n",
        "- PDF_FOLDER: folder on drive where PDFs are stored\n",
        "- CACHE_DIR: where to save cache \n",
        "\n",
        "**Example of drive structure:**\n",
        "```\n",
        "My Drive/\n",
        "â””â”€â”€ medicinali/           <- PDF_FOLDER\n",
        "    â”œâ”€â”€ farmaco1.pdf\n",
        "    â”œâ”€â”€ farmaco2.pdf\n",
        "    â””â”€â”€ ...\n",
        "â””â”€â”€ rag_cache/            <- CACHE_DIR (created automatically)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiN8JuJysGVY",
        "outputId": "51551723-ba18-4945-a2ff-75baa7cc5868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config caricata | PDF trovati: 147\n",
            "Retrieval: TOP_K=10, THRESHOLD=0.3\n",
            "Model â†’ Gen: gpt-4o-mini | Emb: text-embedding-3-small\n",
            "Cache dir: /Users/sanduandrei/Desktop/RAG-Italian-OTC-Medicine/.cache\n"
          ]
        }
      ],
      "source": [
        "# === OpenAI RAG Config (OpenAI-only) ===\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# SDK\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---- API key / client ----\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY non trovato nell'ambiente. Imposta la variabile e riestheseegui la cella.\")\n",
        "\n",
        "client = OpenAI()  \n",
        "\n",
        "# ---- Config ----\n",
        "class Config:\n",
        "    # Paths (adjust as needed)\n",
        "    PDF_FOLDER = '/Users/sanduandrei/Desktop/RAG-Italian-OTC-Medicine/medicinali'\n",
        "    CACHE_DIR  = '/Users/sanduandrei/Desktop/RAG-Italian-OTC-Medicine/.cache'  # cache hidden\n",
        "    # Models\n",
        "    GENERATION_MODEL = 'gpt-4o-mini'\n",
        "    EMBEDDING_MODEL  = 'text-embedding-3-small'  # 1536-dim\n",
        "\n",
        "    # Chunking\n",
        "    CHUNK_SIZE = 500\n",
        "    CHUNK_OVERLAP = 50\n",
        "\n",
        "    # Retrieval\n",
        "    TOP_K = 10\n",
        "    SIMILARITY_THRESHOLD = 0.30\n",
        "\n",
        "    # Batching\n",
        "    BATCH_SIZE = 64\n",
        "    VERBOSE = True\n",
        "\n",
        "    # Cache artifact filenames\n",
        "    EMBEDDINGS_PATH = 'embeddings.npy'\n",
        "    INDEX_PATH      = 'faiss_index.idx'\n",
        "    METADATA_PATH   = 'metadata.pkl'\n",
        "    CHUNKS_PATH     = 'chunks.pkl'  \n",
        "\n",
        "config = Config()\n",
        "\n",
        "# ---- Ensure folders / show status ----\n",
        "os.makedirs(config.CACHE_DIR, exist_ok=True)\n",
        "\n",
        "if os.path.exists(config.PDF_FOLDER):\n",
        "    pdf_count = sum(f.lower().endswith('.pdf') for f in os.listdir(config.PDF_FOLDER))\n",
        "    print(f'Config caricata | PDF trovati: {pdf_count}')\n",
        "    if pdf_count == 0:\n",
        "        print('Nessun PDF trovato nella cartella.')\n",
        "    print(f'Retrieval: TOP_K={config.TOP_K}, THRESHOLD={config.SIMILARITY_THRESHOLD}')\n",
        "    print(f'Model â†’ Gen: {config.GENERATION_MODEL} | Emb: {config.EMBEDDING_MODEL}')\n",
        "    print(f'Cache dir: {config.CACHE_DIR}')\n",
        "else:\n",
        "    print(f'ERRORE: Cartella non trovata: {config.PDF_FOLDER}')\n",
        "    print('Suggerimenti:')\n",
        "    print('1) Verifica che la cartella esista')\n",
        "    print('2) Controlla il percorso (maiuscole/minuscole contano)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174,
          "referenced_widgets": [
            "6bed33725b404c539a64032c7082891e",
            "0b7209dcc71747ea80b4aa0f8256ac6a",
            "7bceb1750e0644ee86ba278f51382da9",
            "d6309bf5506d4bc88365216895451ae9",
            "59fc4aa94a124cfb95c158ef99c2f7ef",
            "2f9b6ab02bf447f2930efe193292e7aa",
            "689b6d02d4fd41edb35f5dd318b6294e",
            "2c83b70637dc434d94f8eb1871aacf3b",
            "0130580221344f1aab95adfa16aff7d1",
            "06d53d7d7e3347e48318667236128c85",
            "7cf570c7a3e448c6b02d9a78a0363742"
          ]
        },
        "id": "OIiWyB-7utYx",
        "outputId": "43afd25b-21ab-4695-85fd-71bcbdcebbdf"
      },
      "outputs": [],
      "source": [
        "# Loading OpenAI chat\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"Key not in env file.\")\n",
        "\n",
        "client = OpenAI()\n",
        "model_name = \"gpt-4o-mini\"  # \n",
        "\n",
        "print(f\"OpenAI model ready: {model_name}\")\n",
        "\n",
        "def generate_with_openai(prompt: str, model: str = model_name, max_tokens: int = 512, temperature: float = 0.3) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil, stat, pathlib\n",
        "\n",
        "CACHE = \"/Users/sanduandrei/Desktop/RAG-Italian-OTC-Medicine/.cache\"\n",
        "\n",
        "def _makedir(p):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def _writable(path):\n",
        "    try:\n",
        "        os.chmod(path, stat.S_IWUSR | stat.S_IRUSR | stat.S_IXUSR)\n",
        "    except Exception:\n",
        "        pass  # best effort\n",
        "\n",
        "p = pathlib.Path(CACHE).expanduser().resolve()\n",
        "print(\"Target cache dir:\", p)\n",
        "\n",
        "if p.exists():\n",
        "    # Make everything writable, then remove\n",
        "    for root, dirs, files in os.walk(p, topdown=False):\n",
        "        for name in files:\n",
        "            _writable(os.path.join(root, name))\n",
        "        for name in dirs:\n",
        "            _writable(os.path.join(root, name))\n",
        "    _writable(str(p))\n",
        "    shutil.rmtree(str(p), ignore_errors=False)\n",
        "    print(\"Removed:\", p)\n",
        "else:\n",
        "    print(\"â„¹Directory did not exist:\", p)\n",
        "\n",
        "_makedir(str(p))\n",
        "print(\"Recreated empty cache dir:\", p)\n",
        "print(\"Contents now:\", os.listdir(str(p)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRctEV2ssGVY"
      },
      "source": [
        "## Cell 4: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JUAm3tfQsdDy",
        "outputId": "43fb8131-f3ca-47bd-aca9-c039b7d8b842"
      },
      "outputs": [],
      "source": [
        "%pip install -q openai python-dotenv PyPDF2 faiss-cpu langchain-text-splitters tqdm\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import faiss  # provided by faiss-cpu\n",
        "\n",
        "# OpenAI SDK + dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/Users/sanduandrei/Desktop/RAG-Italian-OTC-Medicine/.env\")\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "print(\"Libraries imported (OpenAI, FAISS, PyPDF2, text splitters)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-duOQ0qsGVZ"
      },
      "source": [
        "## Cell 5: Text cleaning functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gak-_bPsGVZ",
        "outputId": "9f9c7165-cf39-40f2-b4ad-4f856cb9fbd9"
      },
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Cleans text by removing non-printable characters\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return ''\n",
        "    cleaned = ''.join(c for c in text if c.isprintable() or c == '\\n')\n",
        "    cleaned = ' '.join(cleaned.split())\n",
        "    return cleaned.strip()\n",
        "\n",
        "print('Text cleaning func loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDk9yeb4sGVZ"
      },
      "source": [
        "## ðŸ“„ Cella 6: Extraction PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIkx5ZTVsGVZ",
        "outputId": "15d3b872-8445-40bd-ffa2-06819f8aaca8"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extract clean text from PDF\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        content = []\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                text = page.extract_text()\n",
        "                if text and len(text.strip()) > 20:\n",
        "                    cleaned = clean_text(text)\n",
        "                    if cleaned:\n",
        "                        content.append(cleaned)\n",
        "            except:\n",
        "                continue\n",
        "        return '\\n'.join(content).strip()\n",
        "    except Exception as e:\n",
        "        print(f'PDF reading error: {e}')\n",
        "        return ''\n",
        "\n",
        "print('PDF extraction function loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMjCyDNRsGVZ"
      },
      "source": [
        "## Cell 7: Single PDF Extraction\n",
        "\n",
        "Test extraction on single PDF to verify that everything works "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_pfGzfrsGVZ",
        "outputId": "b25d2621-fd23-4fbd-e9be-a439ce6ee8f1"
      },
      "outputs": [],
      "source": [
        "pdf_files = [f for f in os.listdir(config.PDF_FOLDER) if f.endswith('.pdf')]\n",
        "\n",
        "if pdf_files:\n",
        "    test_pdf = os.path.join(config.PDF_FOLDER, pdf_files[0])\n",
        "    print(f'TEST: {pdf_files[0]}')\n",
        "    result = extract_text_from_pdf(test_pdf)\n",
        "\n",
        "    if result:\n",
        "        print(f'Extracted {len(result):,} characters')\n",
        "        print(f'\\nPreview first 300 char:')\n",
        "        print(result[:300] + '...')\n",
        "    else:\n",
        "        print('No text extracted - verifiy PDF')\n",
        "else:\n",
        "    print('No PDF found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aee4w53CsGVa"
      },
      "source": [
        "## Cell 8: Extraction and chunking all PDFs\n",
        "\n",
        "**IMPORTANT**: This cell:\n",
        "- Processes all PDFs in the folder \n",
        "- Saves results in cache on Google Drive\n",
        "- May take several minutes on the first run\n",
        "- Subsequent runs will be instant "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQFsdsKssGVa",
        "outputId": "b715161c-32be-4cc1-ac3e-d39a034a458a"
      },
      "outputs": [],
      "source": [
        "def extract_and_chunk_all_pdfs(config) -> List[Dict]:\n",
        "    \"\"\"Extracts and splits all PDF files into chunks\"\"\"\n",
        "    chunker = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config.CHUNK_SIZE,\n",
        "        chunk_overlap=config.CHUNK_OVERLAP\n",
        "    )\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(config.PDF_FOLDER) if f.endswith('.pdf')]\n",
        "    print(f'Found {len(pdf_files)} PDFs to process')\n",
        "\n",
        "    all_chunks = []\n",
        "    errors = 0\n",
        "\n",
        "    for pdf_file in tqdm(pdf_files, desc='Processing PDF'):\n",
        "        file_path = os.path.join(config.PDF_FOLDER, pdf_file)\n",
        "        raw_text = extract_text_from_pdf(file_path)\n",
        "\n",
        "        if raw_text and len(raw_text.strip()) > 100:\n",
        "            text_chunks = chunker.split_text(raw_text)\n",
        "            for idx, chunk_text in enumerate(text_chunks):\n",
        "                all_chunks.append({\n",
        "                    'text': chunk_text,\n",
        "                    'document': pdf_file,\n",
        "                    'chunk_id': f'{pdf_file}_{idx}'\n",
        "                })\n",
        "        else:\n",
        "            errors += 1\n",
        "\n",
        "    if errors > 0:\n",
        "        print(f'{errors} PDFs not processed')\n",
        "\n",
        "    print(f'Total chunks created: {len(all_chunks):,}')\n",
        "    return all_chunks\n",
        "\n",
        "# Carica dalla cache o processa\n",
        "chunks_cache = os.path.join(config.CACHE_DIR, 'chunks.pkl')\n",
        "\n",
        "if os.path.exists(chunks_cache):\n",
        "    print('Loading chunks from cache...')\n",
        "    with open(chunks_cache, 'rb') as f:\n",
        "        chunks = pickle.load(f)\n",
        "    print(f'Loaded {len(chunks):,} chunks')\n",
        "else:\n",
        "    print('Cache not found - processing PDFs...')\n",
        "    chunks = extract_and_chunk_all_pdfs(config)\n",
        "\n",
        "    if chunks:\n",
        "        with open(chunks_cache, 'wb') as f:\n",
        "            pickle.dump(chunks, f)\n",
        "        print('Cache saved to Google Drive')\n",
        "    else:\n",
        "        print('Chunks not generated, check PDF')\n",
        "\n",
        "# Statistiche\n",
        "if chunks:\n",
        "    unique_docs = len(set(c['document'] for c in chunks))\n",
        "    print(f'\\nStats:')\n",
        "    print(f'  - Documents: {unique_docs}')\n",
        "    print(f'  - Chunk/document: {len(chunks):,}')\n",
        "    print(f'  - Average chunk/documents: {len(chunks)/unique_docs:.1f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE9ldUWPsGVa"
      },
      "source": [
        "## Cell 9: Embedding Generation\n",
        "\n",
        "**IMPORTANT**: This cell:\n",
        "- Loads multilingual embedding model\n",
        "- Generates vectors for all chunks \n",
        "- Creates FAISS index for fast search\n",
        "- Saves everything to cache on Drive \n",
        "\n",
        "Prima Execution: ~5-10 minuti\n",
        "Riavvii successivi: ~10 secondi (carica dalla cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hkGdlszsGVa",
        "outputId": "565cb5c8-9f3d-44cb-99e1-d0bf6e2ec1dc"
      },
      "outputs": [],
      "source": [
        "# Cell 9 â€” Embedding Generation (OpenAI â†’ FAISS, local cache)\n",
        "\n",
        "import os, pickle, numpy as np, faiss\n",
        "from tqdm.auto import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "client = client if 'client' in globals() else OpenAI()\n",
        "\n",
        "# Local cache paths (saved under your config.CACHE_DIR)\n",
        "emb_cache  = os.path.join(config.CACHE_DIR, 'embeddings.npy')\n",
        "idx_cache  = os.path.join(config.CACHE_DIR, 'faiss_index.idx')\n",
        "meta_cache = os.path.join(config.CACHE_DIR, 'metadata.pkl')   \n",
        "\n",
        "os.makedirs(config.CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def embed_texts_openai(texts, model, batch_size=64):\n",
        "    \"\"\"Return np.ndarray float32 of shape (N, D) using OpenAI embeddings.\"\"\"\n",
        "    vecs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), disable=not config.VERBOSE):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        resp = client.embeddings.create(model=model, input=batch)\n",
        "        vecs.extend([d.embedding for d in resp.data])\n",
        "    return np.array(vecs, dtype=np.float32)\n",
        "\n",
        "# Load from local cache if present\n",
        "if os.path.exists(emb_cache) and os.path.exists(idx_cache) and os.path.exists(meta_cache):\n",
        "    print('Loading embeddings + index + metadata from local cache...')\n",
        "    embeddings = np.load(emb_cache)\n",
        "    index = faiss.read_index(idx_cache)\n",
        "    with open(meta_cache, 'rb') as f:\n",
        "        chunks = pickle.load(f)\n",
        "    print(f'Loaded {len(embeddings):,} vectors (dim={embeddings.shape[1]})')\n",
        "else:\n",
        "    print('Cache not found â€” generating embeddings...')\n",
        "    print(f'Embedding model: {config.EMBEDDING_MODEL}')\n",
        "\n",
        "    # Ensure chunks exist (each item like {'text', 'document', 'chunk_id', ...})\n",
        "    assert \"chunks\" in globals() and len(chunks) > 0, \"No chunks found. Run the PDFâ†’chunk cell first.\"\n",
        "\n",
        "    texts = [c['text'] for c in chunks]\n",
        "    print(f'Embedding {len(texts):,} chunks...')\n",
        "    embeddings = embed_texts_openai(texts, config.EMBEDDING_MODEL, batch_size=config.BATCH_SIZE)\n",
        "\n",
        "    # Cosine similarity via Inner Product on L2-normalized vectors\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    # Save locally (no Google Drive)\n",
        "    np.save(emb_cache, embeddings)\n",
        "    faiss.write_index(index, idx_cache)\n",
        "    with open(meta_cache, 'wb') as f:\n",
        "        pickle.dump(chunks, f)\n",
        "    print('Saved all in cache hidden folder.')\n",
        "\n",
        "print(f'FAISS ready â†’ {index.ntotal:,} vectors | dim={embeddings.shape[1]}')\n",
        "\n",
        "# Tip: to force re-embed later, delete the three cache files above and rerun this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np, faiss\n",
        "print(\"FAISS version:\", faiss.__version__)\n",
        "print(\"FAISS index dim:\", index.d, \"| total vectors:\", index.ntotal)\n",
        "\n",
        "# quick self-check\n",
        "dim = index.d\n",
        "test_vec = np.random.rand(1, dim).astype(np.float32)\n",
        "faiss.normalize_L2(test_vec)\n",
        "distances, indices = index.search(test_vec, 3)\n",
        "print(\"FAISS search test:\", distances, indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK5QAo6xsGVa"
      },
      "source": [
        "## Cell 10: Retrieval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzC_b3PjsGVa",
        "outputId": "dd95fea9-7704-4cb8-91ff-7d520c7b8153"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_chunks(query, top_k=None, threshold=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Retrieves relevant chunks using OpenAI embeddings + FAISS.\n",
        "    Includes safety guards for FAISS crashes on Apple Silicon.\n",
        "    \"\"\"\n",
        "    if top_k is None:\n",
        "        top_k = config.TOP_K\n",
        "    if threshold is None:\n",
        "        threshold = config.SIMILARITY_THRESHOLD\n",
        "\n",
        "    # Ensure FAISS index and chunks exist\n",
        "    if 'index' not in globals() or getattr(index, \"ntotal\", 0) == 0:\n",
        "        raise RuntimeError(\"FAISS index not loaded. Run embedding generation or ensure_faiss_ready().\")\n",
        "    if 'chunks' not in globals() or not chunks:\n",
        "        raise RuntimeError(\"Chunks not loaded. Run the PDFâ†’chunk cell.\")\n",
        "\n",
        "    # Get embedding from OpenAI\n",
        "    resp = client.embeddings.create(model=config.EMBEDDING_MODEL, input=query)\n",
        "    qe = np.array(resp.data[0].embedding, dtype=np.float32)\n",
        "    qe = np.expand_dims(qe, axis=0)\n",
        "    faiss.normalize_L2(qe)\n",
        "\n",
        "    # Double-check dimensionality\n",
        "    if qe.shape[1] != index.d:\n",
        "        raise RuntimeError(\n",
        "            f\"Embedding dimension mismatch: query={qe.shape[1]}, index={index.d}. \"\n",
        "            \"Rebuild the FAISS index with the same embedding model.\"\n",
        "        )\n",
        "\n",
        "    # Create a *copy* of the array to avoid FAISS segfaults on MPS\n",
        "    qe = np.ascontiguousarray(qe)\n",
        "\n",
        "    # Try search safely\n",
        "    try:\n",
        "        distances, indices = index.search(qe, top_k)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"FAISS search failed: {e}\")\n",
        "\n",
        "    results, scores = [], []\n",
        "    for idx, score in zip(indices[0], distances[0]):\n",
        "        if score >= threshold:\n",
        "            results.append(chunks[idx])\n",
        "            scores.append(float(score))\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Found {len(results)}/{top_k} relevant chunks (â‰¥ {threshold})\")\n",
        "\n",
        "    return results, scores\n",
        "\n",
        "print(\"Stable retrieval function loaded for OpenAI + FAISS on CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jwPD-rHsGVb"
      },
      "source": [
        "## Cell 11: Test Retrieval\n",
        "\n",
        "Test the search function with an example question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EPI3fS1sGVb",
        "outputId": "5d985fc3-2497-4c77-b58f-1fe7ef86bafe"
      },
      "outputs": [],
      "source": [
        "test_query = \"Quali sono le controindicazioni della Tachipirina?\"\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "\n",
        "retrieved, scores = retrieve_relevant_chunks(test_query)\n",
        "\n",
        "if not retrieved:\n",
        "    print(\"Nessun risultato sopra la soglia di similaritÃ .\")\n",
        "else:\n",
        "    print(f\"\\n{len(retrieved)} chunk rilevanti trovati:\\n\")\n",
        "    for i, (chunk, score) in enumerate(zip(retrieved, scores), 1):\n",
        "        doc_name = chunk.get(\"document\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjxQvygMsGVb"
      },
      "source": [
        "## Cell 12: Response system (Extractive QA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysZdKoNxsGVb",
        "outputId": "cf26e944-0279-48b0-d6ab-d8bba7c36aa0"
      },
      "outputs": [],
      "source": [
        "# Response system (OpenAI)\n",
        "\n",
        "def answer_question(query, top_k=3, verbose=True):\n",
        "    \"\"\"Responds to query using retrieved chunks + OpenAI generation.\"\"\"\n",
        "    if verbose:\n",
        "        print(f'\\n Question: {query}')\n",
        "        print('='*60)\n",
        "\n",
        "    # 1) RETRIEVAL\n",
        "    retrieved, scores = retrieve_relevant_chunks(query, top_k, verbose=verbose)\n",
        "\n",
        "    if not retrieved:\n",
        "        return {\n",
        "            'query': query,\n",
        "            'answer': 'Non ho trovato informazioni rilevanti nei documenti.',\n",
        "            'sources': [],\n",
        "            'confidence': 0.0\n",
        "        }\n",
        "\n",
        "    # (Optional) lightly truncate each chunk to keep prompt compact\n",
        "    def cut(s, n=2000):  # chars, not tokens\n",
        "        return s if len(s) <= n else s[:n] + \"â€¦\"\n",
        "\n",
        "    # 2) Build context from top-k chunks\n",
        "    context = '\\n\\n---\\n\\n'.join([\n",
        "        f\"Documento: {c.get('document','Sconosciuto')}\\nContenuto: {cut(c['text'])}\"\n",
        "        for c in retrieved[:top_k]\n",
        "    ])\n",
        "\n",
        "    # 3) Prompt for OpenAI\n",
        "    prompt = f\"\"\"Sei un assistente medico esperto. Rispondi alla domanda dell'utente basandoti ESCLUSIVAMENTE sulle informazioni fornite nei documenti.\n",
        "\n",
        "DOCUMENTI:\n",
        "{context}\n",
        "\n",
        "DOMANDA: {query}\n",
        "\n",
        "ISTRUZIONI:\n",
        "- Rispondi in italiano\n",
        "- Usa SOLO le informazioni dei documenti forniti\n",
        "- Se l'informazione non Ã¨ nei documenti, dillo chiaramente\n",
        "- Sii preciso e professionale\n",
        "- Cita il documento da cui prendi l'informazione\n",
        "- Se ci sono discrepanze tra documenti, spiega brevemente\n",
        "\n",
        "RISPOSTA:\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print('\\nðŸ¤– Generazione risposta con OpenAI...')\n",
        "\n",
        "    # 4) Generate with OpenAI\n",
        "    try:\n",
        "        answer = generate_with_openai(\n",
        "            prompt,\n",
        "            model=getattr(config, \"GENERATION_MODEL\", \"gpt-4o-mini\"),\n",
        "            max_tokens=500,\n",
        "            temperature=0.2\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f'Errore generazione: {e}')\n",
        "        answer = \"Errore durante la generazione della risposta.\"\n",
        "\n",
        "    sources = [{'document': c.get('document', 'Sconosciuto'), 'score': s} for c, s in zip(retrieved, scores)]\n",
        "\n",
        "    if verbose:\n",
        "        conf = scores[0] if scores else 0.0\n",
        "        print(f'\\nðŸ’¡ RISPOSTA (score top: {conf:.0%}):')\n",
        "        print('-'*60)\n",
        "        print(answer)\n",
        "        print('-'*60)\n",
        "        fonti = ', '.join([s['document'] for s in sources[:3]])\n",
        "        print(f'\\nFONTI: {fonti}')\n",
        "\n",
        "    return {\n",
        "        'query': query,\n",
        "        'answer': answer,\n",
        "        'sources': sources,\n",
        "        'confidence': scores[0] if scores else 0.0\n",
        "    }\n",
        "\n",
        "print('Sistema QA con OpenAI caricato!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G3mYEMxsGVb"
      },
      "source": [
        "## Cell 13: Test-example Qs\n",
        "\n",
        "Test system with questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej5wRCAcsGVb",
        "outputId": "01bb86c2-a7e0-4c6a-e30b-be663946097d"
      },
      "outputs": [],
      "source": [
        "test_questions = [\n",
        "    \"Posso usare Tachipirina in gravidanza?\",\n",
        "    \"Qual Ã¨ il dosaggio di Tachipirina per adulti?\",\n",
        "    \"Quali sono gli effetti collaterali della Tachipirina?\"\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    result = answer_question(q, verbose=True)\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Optional: show only the final summarized answer nicely\n",
        "    print(f\"Risposta finale:\\n{result['answer']}\\n\")\n",
        "    print(f\"Fonti: {[s['document'] for s in result['sources'][:3]]}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRlQmr2TsGVb"
      },
      "source": [
        "## Cell 14: Interactive Chat \n",
        "\n",
        "**Use:**\n",
        "- Ask questions in natural language\n",
        "- Type 'exit' or 'quit' to exit \n",
        "- Type 'stats' to view system stats "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frxj82PZsGVb",
        "outputId": "4e56f10a-cfbb-45bf-aff1-2fcebeb40add"
      },
      "outputs": [],
      "source": [
        "def interactive_chat():\n",
        "    \"\"\"Interactive RAG Chat using OpenAI\"\"\"\n",
        "    print('\\n' + '='*60)\n",
        "    print('CHAT INTERATTIVA RAG (OpenAI)')\n",
        "    print('='*60)\n",
        "    print('\\nComandi:')\n",
        "    print('  - exit / quit : esci dalla chat')\n",
        "    print('  - stats       : mostra statistiche del sistema')\n",
        "    print('='*60 + '\\n')\n",
        "\n",
        "    query_count = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input('Tu: ').strip()\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            # Exit command\n",
        "            if user_input.lower() in ['exit', 'quit']:\n",
        "                print('\\nArrivederci!')\n",
        "                break\n",
        "\n",
        "            # Stats command\n",
        "            elif user_input.lower() == 'stats':\n",
        "                print(f'\\nSystem stats:')\n",
        "                print(f'  - Queries made: {query_count}')\n",
        "                print(f'  - Total chunks: {len(chunks):,}')\n",
        "                print(f'  - Documents: {len(set(c.get(\"document\",\"?\") for c in chunks))}')\n",
        "                print(f'  - Index size: {index.ntotal:,} vectors')\n",
        "                continue\n",
        "\n",
        "            # Normal question\n",
        "            query_count += 1\n",
        "            print(\"\\nðŸ§  Elaborazione in corso...\\n\")\n",
        "\n",
        "            result = answer_question(user_input, verbose=False)\n",
        "\n",
        "            print('Assistente:')\n",
        "            print('-'*60)\n",
        "            print(result['answer'])\n",
        "            print('-'*60)\n",
        "\n",
        "            if result['sources']:\n",
        "                src = result['sources'][0]\n",
        "                print(f\"Fonte principale: {src['document']} (similaritÃ : {src['score']:.0%})\")\n",
        "\n",
        "            print()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('\\n\\nUscita manuale. A presto!')\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f'\\nErrore: {e}\\n')\n",
        "\n",
        "print('Chat pronta!')\n",
        "print('\\nPer avviare la chat, esegui: interactive_chat()')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzwA8AfesGVb"
      },
      "source": [
        "## Function for Single Questions\n",
        "\n",
        "Single questions, doesn't launch interactive chat. Just for testing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EgyWdbQsGVc",
        "outputId": "59816d3c-1ab1-4166-ba88-41f1cb7de5d9"
      },
      "outputs": [],
      "source": [
        "# Question\n",
        "domanda = \"Quali sono le controindicazioni dell'Aspirina?\"\n",
        "\n",
        "# Response\n",
        "risposta = answer_question(domanda, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17jmXVzZ0wdU",
        "outputId": "8e32d2fd-ba86-41d2-c9a2-e0d49e24c56c"
      },
      "outputs": [],
      "source": [
        "# Edit Q\n",
        "domanda = \"Qual Ã¨ il principio attivo del Moment?\"\n",
        "\n",
        "# Response\n",
        "risposta = answer_question(domanda, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sanity check\n",
        "print(\"answer_question\" in globals())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "qSq3PHAm-XnM",
        "outputId": "8e09af59-594f-4fb5-9be0-f09b28d316a4"
      },
      "outputs": [],
      "source": [
        "# UI with GRADIO\n",
        "\n",
        "# Run after everything else has been loaded on device \n",
        "\n",
        "# 1. Gradio Install \n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# --- CSS ---\n",
        "\n",
        "custom_theme = gr.themes.Soft(\n",
        "    primary_hue=\"emerald\",\n",
        "    secondary_hue=\"teal\"\n",
        ").set(\n",
        "# Correct property names \n",
        "    body_background_fill=\"#ffffff\",    # Sfondo del container principale\n",
        "    body_text_color=\"#212121\",         # Colore del testo principale\n",
        "    background_fill_secondary=\"#f0f2f5\" # Sfondo per aree secondarie (come i messaggi chat)\n",
        ")\n",
        "\n",
        "\n",
        "# Additional CSS\n",
        "custom_css = \"\"\"\n",
        "    body {\n",
        "        font-family: 'Segoe UI', sans-serif; /* Un font piÃ¹ moderno */\n",
        "        background-color: #f0f2f5; /* Sfondo leggermente grigio (si abbina al tema) */\n",
        "    }\n",
        "    .gradio-container {\n",
        "        max-width: 900px; /* Limita la larghezza per una migliore leggibilitÃ  */\n",
        "        margin: auto;\n",
        "        border-radius: 12px; /* Angoli arrotondati */\n",
        "        box-shadow: 0 4px 20px rgba(0,0,0,0.1); /* Ombra discreta */\n",
        "        background-color: white;\n",
        "    }\n",
        "    h1 {\n",
        "        color: #00796b; /* Un verde piÃ¹ scuro per il titolo */\n",
        "        text-align: center;\n",
        "        margin-bottom: 20px;\n",
        "        font-size: 2.5em;\n",
        "        font-weight: 600;\n",
        "    }\n",
        "    .gr-textbox-label {\n",
        "        color: #004d40 !important; /* Colore piÃ¹ scuro per le etichette */\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .gradio-chatmessage {\n",
        "        border-radius: 15px; /* Angoli piÃ¹ arrotondati per i messaggi */\n",
        "        padding: 12px 18px;\n",
        "        margin: 8px 0;\n",
        "    }\n",
        "    .gradio-chatmessage--user {\n",
        "        background-color: #e8f5e9; /* Sfondo verde chiaro per l'utente */\n",
        "        color: #388e3c; /* Testo verde piÃ¹ scuro */\n",
        "    }\n",
        "    .gradio-chatmessage--bot {\n",
        "        background-color: #fce4ec; /* Sfondo rosa chiaro per il bot (richiama il simbolo AIFA?) */\n",
        "        color: #ad1457; /* Testo piÃ¹ scuro */\n",
        "    }\n",
        "    .gr-button {\n",
        "        background-color: #00796b !important; /* Colore bottoni verde AIFA */\n",
        "        color: white !important;\n",
        "        border-radius: 8px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .gr-example-label {\n",
        "        background-color: #f0f4c3 !important; /* Sfondo giallo chiaro per gli esempi */\n",
        "        border-color: #afb42b !important; /* Bordo giallo */\n",
        "        color: #689f38 !important; /* Testo verde per gli esempi */\n",
        "        border-radius: 5px;\n",
        "        font-weight: 500;\n",
        "    }\n",
        "    footer {\n",
        "        visibility: hidden; /* Nasconde il footer \"Built with Gradio\" se vuoi */\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# --- Adapter func ---\n",
        "def gradio_chat_adapter(query, history):\n",
        "    print(f\"Domanda (da UI): {query}\")\n",
        "\n",
        "# Call function\n",
        "    result = answer_question(query, verbose=False) # Assicurati che 'answer_question' esista e sia caricata\n",
        "\n",
        "    answer = result.get('answer', \"Errore: non ho trovato una risposta.\")\n",
        "\n",
        "# Sources \n",
        "    sources = result.get('sources')\n",
        "    if sources:\n",
        "        try:\n",
        "            source_doc = sources[0]['document']\n",
        "            score = sources[0]['score']\n",
        "            answer += f\"\\n\\n*(Fonte: {source_doc} | AffidabilitÃ : {score:.0%})*\"\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            pass # Non fa nulla se le fonti non sono formattate correttamente\n",
        "\n",
        "# Simulate typing effect \n",
        "    for i in range(0, len(answer), 3):\n",
        "        time.sleep(0.01)\n",
        "        yield answer[:i+3]\n",
        "\n",
        "\n",
        "# --- Creation and launch of interface w style ---\n",
        "print(\" Avvio dell'interfaccia Chatbot Medico AIFA con Gradio (stile personalizzato)...\")\n",
        "\n",
        "# gr.ChatInterface Ã¨ il componente che crea la chat\n",
        "iface = gr.ChatInterface(\n",
        "    fn=gradio_chat_adapter,\n",
        "    title=\"âš•ï¸ Chatbot Documenti Medici AIFA (RAG)\",\n",
        "    description=\"Fai domande sui medicinali OTC (Tachipirina, Aspirina, Moment, ecc.). Il sistema risponderÃ  basandosi su informazioni AIFA dai documenti forniti.\",\n",
        "    examples=[\n",
        "        \"Quali sono gli effetti collaterali dell'Aspirina?\",\n",
        "        \"Posso usare Tachipirina in gravidanza?\",\n",
        "        \"Qual Ã¨ il principio attivo del Moment?\",\n",
        "        \"Quali sono le controindicazioni per l'uso dell'Ibuprofene?\"\n",
        "    ],\n",
        "    cache_examples=False,\n",
        "    theme=custom_theme, # custom theme add \n",
        "    css=custom_css      # apply add css \n",
        ")\n",
        "\n",
        "# Avvia l'interfaccia!\n",
        "iface.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfpn4vOJsGVc"
      },
      "source": [
        "## ðŸ—‘ï¸ EXTRA: Pulizia Cache (Opzionale)\n",
        "\n",
        "Esegui questa cella SOLO se vuoi eliminare la cache e riprocessare tutto da zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m76lb2_xsGVc"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def clean_cache():\n",
        "    \"\"\"Elimina la cache per riprocessare tutto\"\"\"\n",
        "    if os.path.exists(config.CACHE_DIR):\n",
        "        response = input('Sei sicuro di voler eliminare la cache? (si/no): ')\n",
        "        if response.lower() in ['si', 'sÃ¬', 'yes', 'y']:\n",
        "            shutil.rmtree(config.CACHE_DIR)\n",
        "            os.makedirs(config.CACHE_DIR, exist_ok=True)\n",
        "            print('Cache eliminata. Riavvia il notebook dalla Cella 8.')\n",
        "        else:\n",
        "            print('Operazione annullata')\n",
        "    else:\n",
        "        print('â„¹Nessuna cache da eliminare')\n",
        "\n",
        "# Decommenta la riga sotto per eliminare la cache\n",
        "# clean_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDcCdv8qsGVc"
      },
      "source": [
        "# Quick Guide\n",
        "\n",
        "### First Execution:\n",
        "\n",
        "1. Run the cells in order from 1 to 9  \n",
        "2. Modify the paths in Cell 3  \n",
        "3. Wait for Cell 9 to finish (~5â€“10 min)  \n",
        "4. Test using Cells 11 and 13  \n",
        "5. Use Cell 14 for the interactive chat or Cell 15 for single questions  \n",
        "\n",
        "---\n",
        "\n",
        "### Subsequent Executions:\n",
        "\n",
        "1. Run Cells 1â€“4 (mount + import)  \n",
        "2. Run Cell 8 (load chunks from cache)  \n",
        "3. Run Cell 9 (load embeddings from cache)  \n",
        "4. Run Cells 10, 12, 14 to use the system  \n",
        "\n",
        "---\n",
        "\n",
        "âš¡ **Total reload time:** ~30 seconds\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Tips:**\n",
        "\n",
        "- The cache is saved on Drive, so it persists across sessions  \n",
        "- You can modify the Parameters in Cell 3 (`TOP_K`, `THRESHOLD`, etc.)  \n",
        "- To add new PDFs, delete the cache (Extra Cell) and rerun Cell 8  \n",
        "\n",
        "---\n",
        "\n",
        "**Common Issues:**\n",
        "\n",
        "- **\"Folder not found\":** Check the path in Cell 3  \n",
        "- **\"No PDF found\":** Make sure the files are actually `.pdf`  \n",
        "- **Memory error:** Reduce `BATCH_SIZE` in Cell 3\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Creato per Google Colab** ðŸš€ | **Versione ottimizzata con cache persistente** ðŸ’¾"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.9.22)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0130580221344f1aab95adfa16aff7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06d53d7d7e3347e48318667236128c85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b7209dcc71747ea80b4aa0f8256ac6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9b6ab02bf447f2930efe193292e7aa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_689b6d02d4fd41edb35f5dd318b6294e",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "2c83b70637dc434d94f8eb1871aacf3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9b6ab02bf447f2930efe193292e7aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59fc4aa94a124cfb95c158ef99c2f7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "689b6d02d4fd41edb35f5dd318b6294e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bed33725b404c539a64032c7082891e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b7209dcc71747ea80b4aa0f8256ac6a",
              "IPY_MODEL_7bceb1750e0644ee86ba278f51382da9",
              "IPY_MODEL_d6309bf5506d4bc88365216895451ae9"
            ],
            "layout": "IPY_MODEL_59fc4aa94a124cfb95c158ef99c2f7ef"
          }
        },
        "7bceb1750e0644ee86ba278f51382da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c83b70637dc434d94f8eb1871aacf3b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0130580221344f1aab95adfa16aff7d1",
            "value": 3
          }
        },
        "7cf570c7a3e448c6b02d9a78a0363742": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6309bf5506d4bc88365216895451ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d53d7d7e3347e48318667236128c85",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7cf570c7a3e448c6b02d9a78a0363742",
            "value": "â€‡3/3â€‡[01:15&lt;00:00,â€‡24.18s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
