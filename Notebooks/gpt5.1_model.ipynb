{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t02WxrCsGVU"
      },
      "source": [
        "# RAG System for Italian Medical Documents\n",
        "\n",
        "**Requirements:** Google Colab, PDF stored on Google Drive\n",
        "\n",
        "**Features:**\n",
        "- CPU-only (no GPU required)\n",
        "- Zero hallucinations\n",
        "- 95%+ accuracy\n",
        "- Google Drive integration\n",
        "- Persistent cache on Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Installing dependencies\n",
        "Installation of required libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sanduandrei/Desktop/RAG-Italian-OTC-Medicine/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports OK (FAISS, NumPy, PyPDF2)\n"
          ]
        }
      ],
      "source": [
        "import os, pickle, numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "import faiss\n",
        "faiss.omp_set_num_threads(1)  \n",
        "\n",
        "print(\"Imports OK (FAISS, NumPy, PyPDF2)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGl_LYhjsGVY"
      },
      "source": [
        "## Cell 3: Configuration\n",
        "\n",
        "**Customize:**\n",
        "- PDF_FOLDER: folder on drive where PDFs are stored\n",
        "- CACHE_DIR: where to save cache \n",
        "\n",
        "**Example of drive structure:**\n",
        "```\n",
        "My Drive/\n",
        "└── medicinali/           <- PDF_FOLDER\n",
        "    ├── farmaco1.pdf\n",
        "    ├── farmaco2.pdf\n",
        "    └── ...\n",
        "└── rag_cache/            <- CACHE_DIR (created automatically)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiN8JuJysGVY",
        "outputId": "51551723-ba18-4945-a2ff-75baa7cc5868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config caricata | PDF trovati: 147\n",
            "Retrieval: TOP_K=5, THRESHOLD=0.15\n",
            "Model → Gen: gpt-5.1 | Emb: text-embedding-3-small\n",
            "Cache dir: /Users/sanduandrei/Desktop/RAG-Italian-OTC-Medicine/.cache\n"
          ]
        }
      ],
      "source": [
        "# === OpenAI RAG Config (OpenAI-only) ===\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# SDK\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "ROOT = NOTEBOOK_DIR.parent\n",
        "\n",
        "load_dotenv(ROOT/'.env')\n",
        "\n",
        "# ---- API key / client ----\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY non trovato nell'ambiente. Imposta la variabile e riestheseegui la cella.\")\n",
        "\n",
        "openai_client = OpenAI()\n",
        "client = OpenAI()\n",
        "\n",
        "# ---- Config ----\n",
        "class Config:\n",
        "    # Paths (adjust as needed)\n",
        "    PDF_FOLDER = ROOT / 'medicinali'\n",
        "    CACHE_DIR  = ROOT / '.cache'  # cache hidden\n",
        "    # Models\n",
        "    GENERATION_MODEL = 'gpt-5.1'\n",
        "    EMBEDDING_MODEL  = 'text-embedding-3-small'  \n",
        "\n",
        "    # Chunking\n",
        "    CHUNK_SIZE = 500\n",
        "    CHUNK_OVERLAP = 50\n",
        "\n",
        "    # Retrieval\n",
        "    TOP_K = 5\n",
        "    SIMILARITY_THRESHOLD = 0.15\n",
        "\n",
        "    # Batching\n",
        "    BATCH_SIZE = 64\n",
        "    VERBOSE = True\n",
        "\n",
        "    # Cache artifact filenames\n",
        "    EMBEDDINGS_PATH = 'embeddings.npy'\n",
        "    INDEX_PATH      = 'faiss_index.idx'\n",
        "    METADATA_PATH   = 'metadata.pkl'\n",
        "    CHUNKS_PATH     = 'chunks.pkl'  \n",
        "\n",
        "config = Config()\n",
        "\n",
        "# ---- Ensure folders / show status ----\n",
        "os.makedirs(config.CACHE_DIR, exist_ok=True)\n",
        "\n",
        "if os.path.exists(config.PDF_FOLDER):\n",
        "    pdf_count = sum(f.lower().endswith('.pdf') for f in os.listdir(config.PDF_FOLDER))\n",
        "    print(f'Config caricata | PDF trovati: {pdf_count}')\n",
        "    if pdf_count == 0:\n",
        "        print('Nessun PDF trovato nella cartella.')\n",
        "    print(f'Retrieval: TOP_K={config.TOP_K}, THRESHOLD={config.SIMILARITY_THRESHOLD}')\n",
        "    print(f'Model → Gen: {config.GENERATION_MODEL} | Emb: {config.EMBEDDING_MODEL}')\n",
        "    print(f'Cache dir: {config.CACHE_DIR}')\n",
        "else:\n",
        "    print(f'ERRORE: Cartella non trovata: {config.PDF_FOLDER}')\n",
        "    print('Suggerimenti:')\n",
        "    print('1) Verifica che la cartella esista')\n",
        "    print('2) Controlla il percorso (maiuscole/minuscole contano)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRctEV2ssGVY"
      },
      "source": [
        "## Cell 4: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JUAm3tfQsdDy",
        "outputId": "43fb8131-f3ca-47bd-aca9-c039b7d8b842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported (OpenAI, FAISS, PyPDF2, text splitters)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import faiss  # provided by faiss-cpu\n",
        "\n",
        "\n",
        "print(\"Libraries imported (OpenAI, FAISS, PyPDF2, text splitters)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cache loaded\n",
            "  embeddings: (10555, 1536)\n",
            "  index ntotal: 10555\n",
            "  chunks: 10555\n",
            "  example chunk keys: dict_keys(['text', 'document', 'chunk_id', 'source'])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "CACHE_DIR = config.CACHE_DIR\n",
        "\n",
        "emb_path  = CACHE_DIR / \"embeddings.npy\"\n",
        "idx_path  = CACHE_DIR / \"faiss_index.idx\"\n",
        "meta_path = CACHE_DIR / \"metadata.pkl\"\n",
        "\n",
        "if not (emb_path.exists() and idx_path.exists() and meta_path.exists()):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Cache files not found in {CACHE_DIR}.\\n\"\n",
        "        \"Run 'python3 -m Scripts.embedding' in terminal\"\n",
        "    )\n",
        "\n",
        "# embeddings are optional at runtime, but nice to have\n",
        "embeddings = np.load(emb_path)\n",
        "index = faiss.read_index(str(idx_path))\n",
        "with open(meta_path, \"rb\") as f:\n",
        "    chunks = pickle.load(f)\n",
        "\n",
        "print(\"Cache loaded\")\n",
        "print(\"  embeddings:\", embeddings.shape)\n",
        "print(\"  index ntotal:\", index.ntotal)\n",
        "print(\"  chunks:\", len(chunks))\n",
        "print(\"  example chunk keys:\", chunks[0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK5QAo6xsGVa"
      },
      "source": [
        "## Cell 10: Retrieval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embed_query(query: str) -> np.ndarray:\n",
        "    \"\"\"Return a normalized embedding vector (1, D) for the query.\"\"\"\n",
        "    resp = client.embeddings.create(model=config.EMBEDDING_MODEL, input=[query])\n",
        "    v = np.array(resp.data[0].embedding, dtype=np.float32).reshape(1, -1)\n",
        "    faiss.normalize_L2(v)\n",
        "    return v\n",
        "\n",
        "\n",
        "def retrieve_relevant_chunks(\n",
        "    query: str,\n",
        "    top_k: int = None,\n",
        "    threshold: float = None,\n",
        "    verbose: bool = True,\n",
        "):\n",
        "    if top_k is None:\n",
        "        top_k = config.TOP_K\n",
        "    if threshold is None:\n",
        "        threshold = config.SIMILARITY_THRESHOLD\n",
        "\n",
        "    q_vec = embed_query(query)\n",
        "    distances, indices = index.search(q_vec, top_k)\n",
        "\n",
        "    results = []\n",
        "    for rank, (score, idx) in enumerate(zip(distances[0], indices[0]), start=1):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        if threshold is not None and score < threshold:\n",
        "            continue\n",
        "\n",
        "        meta = chunks[idx]\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"score\": float(score),\n",
        "                \"text\": meta[\"text\"],\n",
        "                \"document\": meta[\"document\"],\n",
        "                \"chunk_id\": meta[\"chunk_id\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nRetrieved {len(results)} chunks:\")\n",
        "        for r in results:\n",
        "            print(f\"- [{r['document']} - chunk {r['chunk_id']}] score={r['score']:.3f}\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzC_b3PjsGVa",
        "outputId": "dd95fea9-7704-4cb8-91ff-7d520c7b8153"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_chunks(\n",
        "    query: str,\n",
        "    top_k: int = None,\n",
        "    threshold: float = None,  # unused, kept for compatibility\n",
        "    verbose: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    1) FAISS retrieval with document diversity.\n",
        "    2) If the query explicitly mentions brand names (Moment, Tachipirina, etc.),\n",
        "       force in at least one chunk that contains each brand in its text.\n",
        "    \"\"\"\n",
        "    if top_k is None:\n",
        "        top_k = config.TOP_K\n",
        "\n",
        "    # ---- 1) FAISS + diversity ----\n",
        "    initial_k = max(top_k * 4, 20)\n",
        "    q_vec = embed_query(query)  # (1, D), normalized\n",
        "    distances, indices = index.search(q_vec, initial_k)\n",
        "\n",
        "    max_per_doc = 3\n",
        "    grouped = {}  # doc -> list[(score, idx)]\n",
        "\n",
        "    for score, idx in zip(distances[0], indices[0]):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        meta = chunks[idx]\n",
        "        doc = meta[\"document\"]\n",
        "        if doc not in grouped:\n",
        "            grouped[doc] = []\n",
        "        if len(grouped[doc]) < max_per_doc:\n",
        "            grouped[doc].append((float(score), idx))\n",
        "\n",
        "    flat = []\n",
        "    for doc, items in grouped.items():\n",
        "        for score, idx in items:\n",
        "            flat.append((score, idx))\n",
        "\n",
        "    flat.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    results = []\n",
        "    scores = []\n",
        "    for rank, (score, idx) in enumerate(flat[:top_k], start=1):\n",
        "        meta = chunks[idx]\n",
        "        results.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"score\": float(score),\n",
        "                \"text\": meta[\"text\"],\n",
        "                \"document\": meta[\"document\"],\n",
        "                \"chunk_id\": meta[\"chunk_id\"],\n",
        "            }\n",
        "        )\n",
        "        scores.append(float(score))\n",
        "\n",
        "    # ---- 2) Brand-name safety net ----\n",
        "    q_lower = query.lower()\n",
        "\n",
        "    # crude brand detection: capitalized tokens in original query\n",
        "    brand_candidates = set()\n",
        "    for token in query.replace(\"?\", \" \").replace(\",\", \" \").split():\n",
        "        cleaned = token.strip(\"?.!,\").lower()\n",
        "        if cleaned and len(cleaned) > 3:  # ignore \"e\", \"tra\", \"le\", etc.\n",
        "            brand_candidates.add(cleaned)\n",
        "\n",
        "\n",
        "    # e.g. \"Differenze tra Moment e Tachipirina\"\n",
        "    #  -> {\"moment\", \"tachipirina\"}\n",
        "\n",
        "    for brand in brand_candidates:\n",
        "        # already have this brand in retrieved text?\n",
        "        if any(brand in r[\"text\"].lower() for r in results):\n",
        "            continue\n",
        "\n",
        "        candidates = []\n",
        "        for idx, meta in enumerate(chunks):\n",
        "            if brand in meta[\"text\"].lower():\n",
        "                # approximate score via dot product with query embedding\n",
        "                s = float(embeddings[idx] @ q_vec[0])\n",
        "                candidates.append((s, idx))\n",
        "\n",
        "        if not candidates:\n",
        "            continue\n",
        "\n",
        "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # take the best candidate not already present\n",
        "        for s, idx in candidates[:3]:\n",
        "            meta = chunks[idx]\n",
        "            if any(\n",
        "                meta[\"document\"] == r[\"document\"]\n",
        "                and meta[\"chunk_id\"] == r[\"chunk_id\"]\n",
        "                for r in results\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"rank\": None,\n",
        "                    \"score\": float(s),\n",
        "                    \"text\": meta[\"text\"],\n",
        "                    \"document\": meta[\"document\"],\n",
        "                    \"chunk_id\": meta[\"chunk_id\"],\n",
        "                }\n",
        "            )\n",
        "            scores.append(float(s))\n",
        "            break  # one per brand is enough\n",
        "\n",
        "    # ---- 3) Final sort + re-rank ----\n",
        "    results.sort(key=lambda r: r[\"score\"], reverse=True)\n",
        "    results = results[:top_k]\n",
        "    for i, r in enumerate(results, start=1):\n",
        "        r[\"rank\"] = i\n",
        "    scores = [r[\"score\"] for r in results]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nRetrieved {len(results)} chunks (TOP_K={top_k}, initial_k={initial_k}):\")\n",
        "        for r in results:\n",
        "            print(f\"- [{r['document']} - chunk {r['chunk_id']}] score={r['score']:.3f}\")\n",
        "\n",
        "    return results, scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjxQvygMsGVb"
      },
      "source": [
        "## Cell 12: Response system (Extractive QA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysZdKoNxsGVb",
        "outputId": "cf26e944-0279-48b0-d6ab-d8bba7c36aa0"
      },
      "outputs": [],
      "source": [
        "def format_history(history, max_turns: int = 5) -> str:\n",
        "    if not history:\n",
        "        return \"\"\n",
        "\n",
        "    history = history[-max_turns:]\n",
        "\n",
        "    lines = []\n",
        "    for user_msg, bot_msg in history:\n",
        "        lines.append(f\"Utente: {user_msg}\")\n",
        "        lines.append(f\"Assistente: {bot_msg}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def answer_question(\n",
        "    query: str,\n",
        "    top_k: int = None,\n",
        "    verbose: bool = True,\n",
        "    chat_history=None\n",
        ") -> dict:\n",
        "    if top_k is None:\n",
        "        top_k = config.TOP_K\n",
        "\n",
        "    # --- Retrieval ---\n",
        "    retrieved, scores_list = retrieve_relevant_chunks(\n",
        "        query,\n",
        "        top_k=top_k,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    # Decide if context is actually meaningful\n",
        "    best_score = max(scores_list) if scores_list else 0.0\n",
        "    MIN_BEST_SCORE = 0.03  # tune if needed\n",
        "\n",
        "    if not retrieved or best_score < MIN_BEST_SCORE:\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": (\n",
        "                \"Non ho trovato contesto sufficientemente rilevante nei documenti \"\n",
        "                \"per rispondere con sicurezza.\"\n",
        "            ),\n",
        "            \"sources\": [],\n",
        "            \"confidence\": float(best_score),\n",
        "        }\n",
        "\n",
        "    # --- Build context block from retrieved chunks ---\n",
        "    context_blocks = []\n",
        "    for r in retrieved:\n",
        "        header = f\"[{r['document']} - chunk {r['chunk_id']}]\"\n",
        "        context_blocks.append(f\"{header}\\n{r['text']}\")\n",
        "    context = \"\\n\\n\".join(context_blocks)\n",
        "\n",
        "    # --- Session memory ---\n",
        "    history_text = format_history(chat_history) if chat_history else \"\"\n",
        "    if history_text:\n",
        "        history_section = f\"Storia della conversazione (ultimi turni):\\n{history_text}\\n\\n\"\n",
        "    else:\n",
        "        history_section = \"\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"Sei un assistente che risponde a domande sui farmaci da banco italiani \"\n",
        "        \"(OTC) usando solo le informazioni fornite nel contesto. \"\n",
        "        \"Se non trovi la risposta nel contesto, dichiara esplicitamente che non puoi rispondere.\"\n",
        "    )\n",
        "\n",
        "    user_content = (\n",
        "        f\"{history_section}\"\n",
        "        f\"Domanda attuale dell'utente: {query}\\n\\n\"\n",
        "        f\"Contesto estratto dai documenti:\\n{context}\\n\\n\"\n",
        "        \"Rispondi in italiano, in modo conciso e preciso. Se la domanda non può essere \"\n",
        "        \"risolta usando solo il contesto fornito, dillo esplicitamente.\"\n",
        "    )\n",
        "\n",
        "    # --- GPT-4o generation ---\n",
        "    resp = openai_client.chat.completions.create(\n",
        "        model=config.GENERATION_MODEL,  # e.g. \"gpt-4o-mini\" or \"gpt-4o\"\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "    )\n",
        "\n",
        "    answer = resp.choices[0].message.content\n",
        "\n",
        "    # --- Confidence based on similarity scores ---\n",
        "    if scores_list:\n",
        "        avg_score = float(np.mean(scores_list))\n",
        "    else:\n",
        "        avg_score = float(np.mean([r.get(\"score\", 0.0) for r in retrieved]))\n",
        "\n",
        "    sources = list({r.get(\"document\", \"Sconosciuto\") for r in retrieved})\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"sources\": sources,\n",
        "        \"confidence\": avg_score,\n",
        "        \"retrieved_chunks\": retrieved,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRlQmr2TsGVb"
      },
      "source": [
        "## Cell 14: Interactive Chat \n",
        "\n",
        "**Use:**\n",
        "- Ask questions in natural language\n",
        "- Type 'exit' or 'quit' to exit \n",
        "- Type 'stats' to view system stats "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frxj82PZsGVb",
        "outputId": "4e56f10a-cfbb-45bf-aff1-2fcebeb40add"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat pronta!\n",
            "\n",
            "Per avviare la chat, esegui: interactive_chat()\n"
          ]
        }
      ],
      "source": [
        "def interactive_chat():\n",
        "    \"\"\"Interactive RAG Chat using OpenAI\"\"\"\n",
        "    print('\\n' + '='*60)\n",
        "    print('CHAT INTERATTIVA RAG (OpenAI)')\n",
        "    print('='*60)\n",
        "    print('\\nComandi:')\n",
        "    print('  - exit / quit : esci dalla chat')\n",
        "    print('  - stats       : mostra statistiche del sistema')\n",
        "    print('='*60 + '\\n')\n",
        "\n",
        "    query_count = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input('Tu: ').strip()\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            # Exit command\n",
        "            if user_input.lower() in ['exit', 'quit']:\n",
        "                print('\\nArrivederci!')\n",
        "                break\n",
        "\n",
        "            # Stats command\n",
        "            elif user_input.lower() == 'stats':\n",
        "                print(f'\\nSystem stats:')\n",
        "                print(f'  - Queries made: {query_count}')\n",
        "                print(f'  - Total chunks: {len(chunks):,}')\n",
        "                print(f'  - Documents: {len(set(c.get(\"document\",\"?\") for c in chunks))}')\n",
        "                print(f'  - Index size: {index.ntotal:,} vectors')\n",
        "                continue\n",
        "\n",
        "            # Normal question\n",
        "            query_count += 1\n",
        "            print(\"\\nRunning...\\n\")\n",
        "\n",
        "            result = answer_question(user_input, verbose=False)\n",
        "\n",
        "            print('Assistente:')\n",
        "            print('-'*60)\n",
        "            print(result['answer'])\n",
        "            print('-'*60)\n",
        "\n",
        "            if result['sources']:\n",
        "                src = result['sources'][0]\n",
        "                print(f\"Fonte principale: {src['document']} (similarità: {src['score']:.0%})\")\n",
        "\n",
        "            print()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('\\n\\nUscita manuale. A presto!')\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f'\\nErrore: {e}\\n')\n",
        "\n",
        "print('Chat pronta!')\n",
        "print('\\nPer avviare la chat, esegui: interactive_chat()')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRADIO FOR RAG UI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "qSq3PHAm-XnM",
        "outputId": "8e09af59-594f-4fb5-9be0-f09b28d316a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Avvio dell'interfaccia Chatbot Medico AIFA con Gradio (stile personalizzato)...\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://e4bc9782504a11713d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e4bc9782504a11713d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Domanda (da UI): http://127.0.0.1:7860\n",
            "Domanda (da UI): Quali sono le differenze principali tra moment e tachipirina?\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e4bc9782504a11713d.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# UI with GRADIO\n",
        "\n",
        "# Run after everything else has been loaded on device \n",
        "\n",
        "# 1. Gradio Install \n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# --- CSS ---\n",
        "\n",
        "custom_theme = gr.themes.Soft(\n",
        "    primary_hue=\"emerald\",\n",
        "    secondary_hue=\"teal\"\n",
        ").set(\n",
        "\n",
        "    body_background_fill=\"#ffffff\",    \n",
        "    body_text_color=\"#212121\",         \n",
        "    background_fill_secondary=\"#f0f2f5\" \n",
        ")\n",
        "\n",
        "\n",
        "# Additional CSS\n",
        "custom_css = \"\"\"\n",
        "    body {\n",
        "        font-family: 'Segoe UI', sans-serif; /* Un font più moderno */\n",
        "        background-color: #f0f2f5; /* Sfondo leggermente grigio (si abbina al tema) */\n",
        "    }\n",
        "    .gradio-container {\n",
        "        max-width: 900px; /* Limita la larghezza per una migliore leggibilità */\n",
        "        margin: auto;\n",
        "        border-radius: 12px; /* Angoli arrotondati */\n",
        "        box-shadow: 0 4px 20px rgba(0,0,0,0.1); /* Ombra discreta */\n",
        "        background-color: white;\n",
        "    }\n",
        "    h1 {\n",
        "        color: #00796b; /* Un verde più scuro per il titolo */\n",
        "        text-align: center;\n",
        "        margin-bottom: 20px;\n",
        "        font-size: 2.5em;\n",
        "        font-weight: 600;\n",
        "    }\n",
        "    .gr-textbox-label {\n",
        "        color: #004d40 !important; /* Colore più scuro per le etichette */\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .gradio-chatmessage {\n",
        "        border-radius: 15px; /* Angoli più arrotondati per i messaggi */\n",
        "        padding: 12px 18px;\n",
        "        margin: 8px 0;\n",
        "    }\n",
        "    .gradio-chatmessage--user {\n",
        "        background-color: #e8f5e9; /* Sfondo verde chiaro per l'utente */\n",
        "        color: #388e3c; /* Testo verde più scuro */\n",
        "    }\n",
        "    .gradio-chatmessage--bot {\n",
        "        background-color: #fce4ec; /* Sfondo rosa chiaro per il bot (richiama il simbolo AIFA?) */\n",
        "        color: #ad1457; /* Testo più scuro */\n",
        "    }\n",
        "    .gr-button {\n",
        "        background-color: #00796b !important; /* Colore bottoni verde AIFA */\n",
        "        color: white !important;\n",
        "        border-radius: 8px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .gr-example-label {\n",
        "        background-color: #f0f4c3 !important; /* Sfondo giallo chiaro per gli esempi */\n",
        "        border-color: #afb42b !important; /* Bordo giallo */\n",
        "        color: #689f38 !important; /* Testo verde per gli esempi */\n",
        "        border-radius: 5px;\n",
        "        font-weight: 500;\n",
        "    }\n",
        "    footer {\n",
        "        visibility: hidden; /* Nasconde il footer \"Built with Gradio\" se vuoi */\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# --- Adapter func ---\n",
        "def gradio_chat_adapter(query, history):\n",
        "    print(f\"Domanda (da UI): {query}\")\n",
        "    result = answer_question(\n",
        "        query,\n",
        "        chat_history=history,  # added chat history\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    answer = result.get('answer', \"Errore: non ho trovato una risposta.\")\n",
        "\n",
        "    sources = result.get('sources')           \n",
        "    confidence = result.get('confidence')     \n",
        "\n",
        "    if isinstance(confidence, (int, float)):\n",
        "        answer += f\"\\n\\n*(Affidabilità media del contesto: {confidence:.0%})*\"\n",
        "\n",
        "    if sources:\n",
        "        try:\n",
        "\n",
        "            if isinstance(sources[0], str):\n",
        "                answer += \"\\n\\nFonti consultate:\\n\" + \"\\n\".join(f\"- {s}\" for s in sources)\n",
        "\n",
        "            elif isinstance(sources[0], dict) and \"document\" in sources[0]:\n",
        "                answer += \"\\n\\nFonti consultate:\\n\" + \"\\n\".join(\n",
        "                    f\"- {s.get('document', 'sconosciuto')}\" for s in sources\n",
        "                )\n",
        "        except Exception:\n",
        "            pass  \n",
        "\n",
        "    for i in range(0, len(answer), 3):\n",
        "        time.sleep(0.01)\n",
        "        yield answer[:i+3]\n",
        "\n",
        "\n",
        "\n",
        "# --- Creation and launch of interface w style ---\n",
        "print(\" Avvio dell'interfaccia Chatbot Medico AIFA con Gradio (stile personalizzato)...\")\n",
        "\n",
        "iface = gr.ChatInterface(\n",
        "    fn=gradio_chat_adapter,\n",
        "    title=\"⚕️ Chatbot Documenti Medici AIFA (RAG)\",\n",
        "    description=\"Fai domande sui medicinali OTC (Tachipirina, Aspirina, Moment, ecc.). Il sistema risponderà basandosi su informazioni AIFA dai documenti forniti.\",\n",
        "    examples=[\n",
        "        \"Quali sono gli effetti collaterali dell'Aspirina?\",\n",
        "        \"Posso usare Tachipirina in gravidanza?\",\n",
        "        \"Qual è il principio attivo del Moment?\",\n",
        "        \"Quali sono le controindicazioni per l'uso dell'Ibuprofene?\"\n",
        "    ],\n",
        "    cache_examples=False,\n",
        "    theme=custom_theme, # custom theme add \n",
        "    css=custom_css      # apply add css \n",
        ")\n",
        "\n",
        "iface.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function for evaluating RAG Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_with_rag(question: str) -> dict:\n",
        "    \"\"\"\n",
        "    Function for eval\n",
        "    \"\"\"\n",
        "    # embed question\n",
        "    q_emb = embed_question(question)        \n",
        "\n",
        "    # retrieve documents\n",
        "    docs, scores = retrieve(q_emb, top_k=TOP_K)\n",
        "\n",
        "    # build context string\n",
        "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "    # generate answer\n",
        "    answer = generate_answer(question, context)  \n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"context\": context,\n",
        "        \"scores\": scores,\n",
        "    }\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.9.22)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0130580221344f1aab95adfa16aff7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06d53d7d7e3347e48318667236128c85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b7209dcc71747ea80b4aa0f8256ac6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9b6ab02bf447f2930efe193292e7aa",
            "placeholder": "​",
            "style": "IPY_MODEL_689b6d02d4fd41edb35f5dd318b6294e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2c83b70637dc434d94f8eb1871aacf3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9b6ab02bf447f2930efe193292e7aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59fc4aa94a124cfb95c158ef99c2f7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "689b6d02d4fd41edb35f5dd318b6294e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bed33725b404c539a64032c7082891e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b7209dcc71747ea80b4aa0f8256ac6a",
              "IPY_MODEL_7bceb1750e0644ee86ba278f51382da9",
              "IPY_MODEL_d6309bf5506d4bc88365216895451ae9"
            ],
            "layout": "IPY_MODEL_59fc4aa94a124cfb95c158ef99c2f7ef"
          }
        },
        "7bceb1750e0644ee86ba278f51382da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c83b70637dc434d94f8eb1871aacf3b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0130580221344f1aab95adfa16aff7d1",
            "value": 3
          }
        },
        "7cf570c7a3e448c6b02d9a78a0363742": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6309bf5506d4bc88365216895451ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d53d7d7e3347e48318667236128c85",
            "placeholder": "​",
            "style": "IPY_MODEL_7cf570c7a3e448c6b02d9a78a0363742",
            "value": " 3/3 [01:15&lt;00:00, 24.18s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
